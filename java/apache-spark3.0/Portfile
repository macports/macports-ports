# -*- coding: utf-8; mode: tcl; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- vim:fenc=utf-8:ft=tcl:et:sw=4:ts=4:sts=4
PortSystem          1.0
PortGroup           java 1.0
PortGroup           select 1.0

name                apache-spark3.0
version             3.0.0
revision            0
categories          java parallel devel lang databases
maintainers         nomaintainer
description         Engine for large-scale data processing
long_description \
    Apache Spark is a lightning-fast unified analytics engine for big data and machine \
    learning. It was originally developed at UC Berkeley in 2009. Apache Spark is a fast \
    and general-purpose cluster computing system. It provides high-level APIs in Java, \
    Scala and Python, and an optimized engine that supports general execution graphs. \
    It also supports a rich set of higher-level tools including Spark SQL for SQL and \
    structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.
homepage            https://spark.apache.org/
platforms           darwin
supported_archs     noarch
license             Apache-2
use_xcode           no

master_sites        http://apache.mirrors.hoobly.com/spark/spark-${version}/ \
                    http://apache.mirrors.pair.com/spark/spark-${version}/ \
                    http://apache.spinellicreations.com/spark/spark-${version}/ \
                    http://mirror.cc.columbia.edu/pub/software/apache/spark/spark-${version}/ \
                    http://mirror.cogentco.com/pub/apache/spark/spark-${version}/ \
                    http://mirror.metrocast.net/apache/spark/spark-${version}/ \
                    http://mirrors.advancedhosters.com/apache/spark/spark-${version}/ \
                    http://mirrors.ibiblio.org/apache/spark/spark-${version}/ \
                    http://www.gtlib.gatech.edu/pub/apache/spark/spark-${version}/ \
                    http://www.trieuvan.com/apache/spark/spark-${version}/ \
                    https://apache.claz.org/spark/spark-${version}/ \
                    https://apache.cs.utah.edu/spark/spark-${version}/ \
                    https://apache.mirrors.lucidnetworks.net/spark/spark-${version}/ \
                    https://apache.osuosl.org/spark/spark-${version}/ \
                    https://ftp.wayne.edu/apache/spark/spark-${version}/ \
                    https://mirror.olnevhost.net/pub/apache/spark/spark-${version}/ \
                    https://mirrors.gigenet.com/apache/spark/spark-${version}/ \
                    https://mirrors.koehn.com/apache/spark/spark-${version}/ \
                    https://mirrors.ocf.berkeley.edu/apache/spark/spark-${version}/ \
                    https://mirrors.sonic.net/apache/spark/spark-${version}/ \
                    https://us.mirrors.quenda.co/apache/spark/spark-${version}/ \
                    https://archive.apache.org/dist/spark/spark-${version}/
distname            spark-${version}-bin-hadoop3.2
distfiles           ${distname}.tgz
extract.suffix      .tgz
checksums           md5    c5c0af4b4b9cb21d214c7026439df236\
                    sha1   580d70c11169f3c54b3fa3f3e7b93205c1dd940d\
                    rmd160 1fb157708751255fdc8dea32cffedd5516f3fc4e\
                    sha256 3c9bef2d002d706b5331415884d3f890ecfdd7c6a692f36ed7a981ad120b2482\
                    size   224453229

use_configure       no

# Require java version
java.version        11+
# JDK port to install if required java not found
java.fallback       openjdk11

depends_run         port:sbt port:scala2.13 port:python38

extract.suffix      .tgz
extract.mkdir       yes

if {$subport == $name} {
    depends_lib          port:apache-spark_select
    select.group         apache-spark
    select.file          ${filespath}/${name}

    build               { }

    destroot {
        set sparkdir ${prefix}/share/${name}

        xinstall -m 755 -d ${sparkdir}
    
    	file copy ${worksrcpath}/${distname} ${destroot}${sparkdir}
	
	    set beeline.cmd beeline
    	set docker-image-tool.cmd docker-image-tool.sh
	    set find-spark-home.cmd find-spark-home
    	set load-spark-env.cmd load-spark-env.sh
    	set pyspark.cmd pyspark
	    set spark-class.cmd spark-class
    	set spark-shell.cmd spark-shell
	    set spark-sql.cmd spark-sql
    	set spark-submit.cmd spark-submit
	    set sparkR.cmd sparkR
	
    	xinstall -m 755 ${worksrcpath}/${distname}/bin/${beeline.cmd} ${destroot}${sparkdir}/bin
	    xinstall -m 755 ${worksrcpath}/${distname}/bin/${docker-image-tool.cmd} ${destroot}${sparkdir}/bin
    	xinstall -m 755 ${worksrcpath}/${distname}/bin/${find-spark-home.cmd} ${destroot}${sparkdir}/bin
	    xinstall -m 755 ${worksrcpath}/${distname}/bin/${load-spark-env.cmd} ${destroot}${sparkdir}/bin
    	xinstall -m 755 ${worksrcpath}/${distname}/bin/${pyspark.cmd} ${destroot}${sparkdir}/bin
	    xinstall -m 755 ${worksrcpath}/${distname}/bin/${spark-class.cmd} ${destroot}${sparkdir}/bin
    	xinstall -m 755 ${worksrcpath}/${distname}/bin/${spark-shell.cmd} ${destroot}${sparkdir}/bin
	    xinstall -m 755 ${worksrcpath}/${distname}/bin/${spark-sql.cmd} ${destroot}${sparkdir}/bin
    	xinstall -m 755 ${worksrcpath}/${distname}/bin/${spark-submit.cmd} ${destroot}${sparkdir}/bin
	    xinstall -m 755 ${worksrcpath}/${distname}/bin/${sparkR.cmd} ${destroot}${sparkdir}/bin
    }
}

notes "
The Spark Home environment variable requires the environment variables to be exported and
set as well as the PySpark Python version. Please add the following lines to your .profile
or export in your shell session to make ${name} work:

    export SPARK_HOME=${prefix}/share/${name}
    export PYSPARK_PYTHON=python3
"
