# -*- coding: utf-8; mode: tcl; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- vim:fenc=utf-8:ft=tcl:et:sw=4:ts=4:sts=4
PortSystem          1.0
PortGroup           java 1.0
PortGroup           select 1.0

name                apache-spark2.4
version             2.4.6
revision            0
categories          java parallel devel lang databases
maintainers         nomaintainer
description         Engine for large-scale data processing
long_description \
    Apache Spark is a lightning-fast unified analytics engine for big data and machine \
    learning. It was originally developed at UC Berkeley in 2009. Apache Spark is a fast \
    and general-purpose cluster computing system. It provides high-level APIs in Java, \
    Scala and Python, and an optimized engine that supports general execution graphs. \
    It also supports a rich set of higher-level tools including Spark SQL for SQL and \
    structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.
homepage            https://spark.apache.org/
platforms           darwin
supported_archs     noarch
license             Apache-2
use_xcode           no

master_sites        http://apache.mirrors.hoobly.com/spark/spark-${version}/ \
                    http://apache.mirrors.pair.com/spark/spark-${version}/ \
                    http://apache.spinellicreations.com/spark/spark-${version}/ \
                    http://mirror.cc.columbia.edu/pub/software/apache/spark/spark-${version}/ \
                    http://mirror.cogentco.com/pub/apache/spark/spark-${version}/ \
                    http://mirror.metrocast.net/apache/spark/spark-${version}/ \
                    http://mirrors.advancedhosters.com/apache/spark/spark-${version}/ \
                    http://mirrors.ibiblio.org/apache/spark/spark-${version}/ \
                    http://www.gtlib.gatech.edu/pub/apache/spark/spark-${version}/ \
                    http://www.trieuvan.com/apache/spark/spark-${version}/ \
                    https://apache.claz.org/spark/spark-${version}/ \
                    https://apache.cs.utah.edu/spark/spark-${version}/ \
                    https://apache.mirrors.lucidnetworks.net/spark/spark-${version}/ \
                    https://apache.osuosl.org/spark/spark-${version}/ \
                    https://ftp.wayne.edu/apache/spark/spark-${version}/ \
                    https://mirror.olnevhost.net/pub/apache/spark/spark-${version}/ \
                    https://mirrors.gigenet.com/apache/spark/spark-${version}/ \
                    https://mirrors.koehn.com/apache/spark/spark-${version}/ \
                    https://mirrors.ocf.berkeley.edu/apache/spark/spark-${version}/ \
                    https://mirrors.sonic.net/apache/spark/spark-${version}/ \
                    https://us.mirrors.quenda.co/apache/spark/spark-${version}/ \
                    https://archive.apache.org/dist/spark/spark-${version}/
distname            spark-${version}-bin-hadoop2.7
distfiles           ${distname}.tgz
extract.suffix      .tgz
checksums           md5    82364f8765d03dfb14cb9c606d678058\
                    sha1   9135002c4027a8280fa60907d86e3dba123d9755\
                    rmd160 d872ce73eb4f98e8c05c2349090833549490e8f0\
                    sha256 7174fee30057fbf226698ebfcfa7c2944a26197e149053aa29b26b250d1e1eba\
                    size   233215067

use_configure       no

# Require java version
java.version        8+
# JDK port to install if required java not found
java.fallback       openjdk8

depends_run         port:sbt port:scala2.12 port:python37

extract.suffix      .tgz
extract.mkdir       yes

if {$subport == $name} {
    depends_lib          port:apache-spark_select
    select.group         apache-spark
    select.file          ${filespath}/${name}

    build               { }

    destroot {
        set sparkdir ${prefix}/share/${name}

        xinstall -m 755 -d ${sparkdir}
    
    	file copy ${worksrcpath}/${distname} ${destroot}${sparkdir}
	
	    set beeline.cmd beeline
    	set docker-image-tool.cmd docker-image-tool.sh
	    set find-spark-home.cmd find-spark-home
    	set load-spark-env.cmd load-spark-env.sh
    	set pyspark.cmd pyspark
	    set spark-class.cmd spark-class
    	set spark-shell.cmd spark-shell
	    set spark-sql.cmd spark-sql
    	set spark-submit.cmd spark-submit
	    set sparkR.cmd sparkR
	
    	xinstall -m 755 ${worksrcpath}/${distname}/bin/${beeline.cmd} ${destroot}${sparkdir}/bin
	    xinstall -m 755 ${worksrcpath}/${distname}/bin/${docker-image-tool.cmd} ${destroot}${sparkdir}/bin
    	xinstall -m 755 ${worksrcpath}/${distname}/bin/${find-spark-home.cmd} ${destroot}${sparkdir}/bin
	    xinstall -m 755 ${worksrcpath}/${distname}/bin/${load-spark-env.cmd} ${destroot}${sparkdir}/bin
    	xinstall -m 755 ${worksrcpath}/${distname}/bin/${pyspark.cmd} ${destroot}${sparkdir}/bin
	    xinstall -m 755 ${worksrcpath}/${distname}/bin/${spark-class.cmd} ${destroot}${sparkdir}/bin
    	xinstall -m 755 ${worksrcpath}/${distname}/bin/${spark-shell.cmd} ${destroot}${sparkdir}/bin
	    xinstall -m 755 ${worksrcpath}/${distname}/bin/${spark-sql.cmd} ${destroot}${sparkdir}/bin
    	xinstall -m 755 ${worksrcpath}/${distname}/bin/${spark-submit.cmd} ${destroot}${sparkdir}/bin
	    xinstall -m 755 ${worksrcpath}/${distname}/bin/${sparkR.cmd} ${destroot}${sparkdir}/bin
    }
}

notes "
The Spark Home environment variable requires the environment variables to be exported and
set as well as the PySpark Python version. Please add the following lines to your .profile
or export in your shell session to make ${name} work:

    export SPARK_HOME=${prefix}/share/${name}
    export PYSPARK_PYTHON=python3
"
