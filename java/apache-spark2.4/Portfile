# -*- coding: utf-8; mode: tcl; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- vim:fenc=utf-8:ft=tcl:et:sw=4:ts=4:sts=4
PortSystem          1.0
PortGroup           java 1.0
PortGroup           select 1.0

name                apache-spark2.4
version             2.4.7
revision            0
categories          java parallel devel lang databases
maintainers         nomaintainer
description         Engine for large-scale data processing
long_description \
    Apache Spark is a lightning-fast unified analytics engine for big data and machine \
    learning. It was originally developed at UC Berkeley in 2009. Apache Spark is a fast \
    and general-purpose cluster computing system. It provides high-level APIs in Java, \
    Scala and Python, and an optimized engine that supports general execution graphs. \
    It also supports a rich set of higher-level tools including Spark SQL for SQL and \
    structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.
homepage            https://spark.apache.org/
platforms           darwin
supported_archs     noarch
license             Apache-2
use_xcode           no

master_sites        http://apache.mirrors.hoobly.com/spark/spark-${version}/ \
                    http://apache.mirrors.pair.com/spark/spark-${version}/ \
                    http://apache.spinellicreations.com/spark/spark-${version}/ \
                    http://mirror.cc.columbia.edu/pub/software/apache/spark/spark-${version}/ \
                    http://mirror.cogentco.com/pub/apache/spark/spark-${version}/ \
                    http://mirror.metrocast.net/apache/spark/spark-${version}/ \
                    http://mirrors.advancedhosters.com/apache/spark/spark-${version}/ \
                    http://mirrors.ibiblio.org/apache/spark/spark-${version}/ \
                    http://www.gtlib.gatech.edu/pub/apache/spark/spark-${version}/ \
                    http://www.trieuvan.com/apache/spark/spark-${version}/ \
                    https://apache.claz.org/spark/spark-${version}/ \
                    https://apache.cs.utah.edu/spark/spark-${version}/ \
                    https://apache.mirrors.lucidnetworks.net/spark/spark-${version}/ \
                    https://apache.osuosl.org/spark/spark-${version}/ \
                    https://ftp.wayne.edu/apache/spark/spark-${version}/ \
                    https://mirror.olnevhost.net/pub/apache/spark/spark-${version}/ \
                    https://mirrors.gigenet.com/apache/spark/spark-${version}/ \
                    https://mirrors.koehn.com/apache/spark/spark-${version}/ \
                    https://mirrors.ocf.berkeley.edu/apache/spark/spark-${version}/ \
                    https://mirrors.sonic.net/apache/spark/spark-${version}/ \
                    https://us.mirrors.quenda.co/apache/spark/spark-${version}/ \
                    https://archive.apache.org/dist/spark/spark-${version}/
distname            spark-${version}-bin-hadoop2.7
distfiles           ${distname}.tgz
extract.suffix      .tgz
checksums           md5    76afb611aaac5721c9fa91fdc9defa99\
                    sha1   75b755b8fe55404593dbd1f5069c605ee89b6ab6\
                    rmd160 a611dafda70c099f8fc57c1df7db73ec9555f419\
                    sha256 13098490936c9931beda3acc4c30cdc5ca707acd1415eebde1030b11903934fe\
                    size   233333392

use_configure       no

# Require java version
java.version        8+
# JDK port to install if required java not found
java.fallback       openjdk8

depends_run         port:sbt port:scala2.12 port:python37

extract.suffix      .tgz
extract.mkdir       yes

if {$subport == $name} {
    depends_lib          port:apache-spark_select
    select.group         apache-spark
    select.file          ${filespath}/${name}

    build               { }

    destroot {
        set sparkdir ${prefix}/share/${name}

        xinstall -m 755 -d ${sparkdir}
    
    	file copy ${worksrcpath}/${distname} ${destroot}${sparkdir}
	
	    set beeline.cmd beeline
    	set docker-image-tool.cmd docker-image-tool.sh
	    set find-spark-home.cmd find-spark-home
    	set load-spark-env.cmd load-spark-env.sh
    	set pyspark.cmd pyspark
	    set spark-class.cmd spark-class
    	set spark-shell.cmd spark-shell
	    set spark-sql.cmd spark-sql
    	set spark-submit.cmd spark-submit
	    set sparkR.cmd sparkR
	
    	xinstall -m 755 ${worksrcpath}/${distname}/bin/${beeline.cmd} ${destroot}${sparkdir}/bin
	    xinstall -m 755 ${worksrcpath}/${distname}/bin/${docker-image-tool.cmd} ${destroot}${sparkdir}/bin
    	xinstall -m 755 ${worksrcpath}/${distname}/bin/${find-spark-home.cmd} ${destroot}${sparkdir}/bin
	    xinstall -m 755 ${worksrcpath}/${distname}/bin/${load-spark-env.cmd} ${destroot}${sparkdir}/bin
    	xinstall -m 755 ${worksrcpath}/${distname}/bin/${pyspark.cmd} ${destroot}${sparkdir}/bin
	    xinstall -m 755 ${worksrcpath}/${distname}/bin/${spark-class.cmd} ${destroot}${sparkdir}/bin
    	xinstall -m 755 ${worksrcpath}/${distname}/bin/${spark-shell.cmd} ${destroot}${sparkdir}/bin
	    xinstall -m 755 ${worksrcpath}/${distname}/bin/${spark-sql.cmd} ${destroot}${sparkdir}/bin
    	xinstall -m 755 ${worksrcpath}/${distname}/bin/${spark-submit.cmd} ${destroot}${sparkdir}/bin
	    xinstall -m 755 ${worksrcpath}/${distname}/bin/${sparkR.cmd} ${destroot}${sparkdir}/bin
    }
}

notes "
The Spark Home environment variable requires the environment variables to be exported and
set as well as the PySpark Python version. Please add the following lines to your .profile
or export in your shell session to make ${name} work:

    export SPARK_HOME=${prefix}/share/${name}
    export PYSPARK_PYTHON=python3
"
