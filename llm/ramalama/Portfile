# -*- coding: utf-8; mode: tcl; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- vim:fenc=utf-8:ft=tcl:et:sw=4:ts=4:sts=4

PortSystem          1.0

PortGroup           github 1.0
PortGroup           python 1.0

github.setup        containers ramalama 0.12.2 v
github.tarball_from archive
revision            1
checksums           rmd160  cd45573d0a9425c83e814b40d615edf814edc927 \
                    sha256  2feb67a5b3c62a1ad58de7803eed42701fe9e5e8cfaca90d853a590d5a1ecc9e \
                    size    822829

homepage            https://ramalama.ai/
license             MIT
description         A tool to simplify the use of local AI models
long_description    \
    Ramalama is an open-source developer tool that simplifies the local serving \
    of AI models from any source and facilitates their use for inference in \
    production, all through the familiar language of containers.

maintainers         {cal @neverpanic} openmaintainer
categories          llm science
supported_archs     noarch

python.default_version  313

depends_run-append  \
                    port:krunkit \
                    port:podman \
                    port:py${python.version}-yaml

notes \
    "${name} defaults to running AI models in podman containers in a podman\
    machine (i.e., VM) started by libkrun. This is not the podman default, so\
    you will have to change it, either by exporting the\
    CONTAINERS_MACHINE_PROVIDER=libkrun environment variable, or by adding\
    'provider = \"libkrun\"' to the '\[machine]' section of\
    '\$HOME/.config/containers/containers.conf'. See man 7 ramalama-macos for\
    more information."
