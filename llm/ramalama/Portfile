# -*- coding: utf-8; mode: tcl; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- vim:fenc=utf-8:ft=tcl:et:sw=4:ts=4:sts=4

PortSystem          1.0

PortGroup           github 1.0
PortGroup           python 1.0

github.setup        containers ramalama 0.8.3 v
github.tarball_from archive
checksums           rmd160  2bbffb2b0133ddbe6d168d1b8c59bd18393b81f2 \
                    sha256  c15c1d2999badc10c18b53add1e37af6398b877aedfdb7317e0e517711481324 \
                    size    504468

homepage            https://ramalama.ai/
license             MIT
description         A tool to simplify the use of local AI models
long_description    \
    Ramalama is an open-source developer tool that simplifies the local serving \
    of AI models from any source and facilitates their use for inference in \
    production, all through the familiar language of containers.

maintainers         {cal @neverpanic} openmaintainer
categories          llm science
supported_archs     noarch

python.default_version  313

depends_run-append  \
                    port:krunkit \
                    port:podman

notes \
    "${name} defaults to running AI models in podman containers in a podman\
    machine (i.e., VM) started by libkrun. This is not the podman default, so\
    you will have to change it, either by exporting the\
    CONTAINERS_MACHINE_PROVIDER=libkrun environment variable, or by adding\
    'provider = \"libkrun\"' to the '\[machine]' section of\
    '\$HOME/.config/containers/containers.conf'. See man 7 ramalama-macos for\
    more information."
