# -*- coding: utf-8; mode: tcl; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- vim:fenc=utf-8:ft=tcl:et:sw=4:ts=4:sts=4

PortSystem          1.0

PortGroup           github 1.0
PortGroup           python 1.0

github.setup        containers ramalama 0.15.0 v
github.tarball_from archive
revision            0
checksums           rmd160  d56bcfb6dbf5aac9733812a1e9ab4b1d07253eea \
                    sha256  d2b77a4434ffb9e359b8d333bb64f1db5d88bf85da0bf67e76f9d0170567f80a \
                    size    1662054

homepage            https://ramalama.ai/
license             MIT
description         A tool to simplify the use of local AI models
long_description    \
    Ramalama is an open-source developer tool that simplifies the local serving \
    of AI models from any source and facilitates their use for inference in \
    production, all through the familiar language of containers.

maintainers         {cal @neverpanic} openmaintainer
categories          llm science
supported_archs     noarch

python.default_version  313

depends_run-append  \
                    port:krunkit \
                    port:podman \
                    port:py${python.version}-jinja2 \
                    port:py${python.version}-yaml

notes \
    "${name} defaults to running AI models in podman containers in a podman\
    machine (i.e., VM) started by libkrun. This is not the podman default, so\
    you will have to change it, either by exporting the\
    CONTAINERS_MACHINE_PROVIDER=libkrun environment variable, or by adding\
    'provider = \"libkrun\"' to the '\[machine]' section of\
    '\$HOME/.config/containers/containers.conf'. See man 7 ramalama-macos for\
    more information."
