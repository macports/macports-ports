From c424c947cac43c236b7ccbbf9d1ea0ae16d303ab Mon Sep 17 00:00:00 2001
From: Sergey Fedorov <vital.had@gmail.com>
Date: Fri, 28 Oct 2022 20:42:43 +0800
Subject: [PATCH 1/8] asmlong.h: add PPC implementation

---
 src/runtime/Integer/asmlong.h | 106 ++++++++++++++++++++++++++++++++++
 1 file changed, 106 insertions(+)

diff --git a/src/runtime/Integer/asmlong.h b/src/runtime/Integer/asmlong.h
index 5303ba3..d1f38a0 100644
--- a/src/runtime/Integer/asmlong.h
+++ b/src/runtime/Integer/asmlong.h
@@ -558,3 +558,109 @@
 	     + (((signed long int) __m1 >> 31) & __m0));		\
   } while (0)
 #endif /* __vax__ */
+
+#if (__ppc__)
+#define add_ssaaaa(sh, sl, ah, al, bh, bl) 									\
+  do {																		\
+    if (__builtin_constant_p (bh) && (bh) == 0)								\
+      __asm__ ("{a%I4|add%I4c} %1,%3,%4\n\t{aze|addze} %0,%2"				\
+	     : "=r" (sh), "=&r" (sl) : "r" (ah), "%r" (al), "rI" (bl));			\
+    else if (__builtin_constant_p (bh) && (bh) == ~(unsigned long int) 0)	\
+      __asm__ ("{a%I4|add%I4c} %1,%3,%4\n\t{ame|addme} %0,%2"				\
+	     : "=r" (sh), "=&r" (sl) : "r" (ah), "%r" (al), "rI" (bl));			\
+    else																	\
+      __asm__ ("{a%I5|add%I5c} %1,%4,%5\n\t{ae|adde} %0,%2,%3"				\
+	     : "=r" (sh), "=&r" (sl)											\
+	     : "%r" (ah), "r" (bh), "%r" (al), "rI" (bl));						\
+  } while (0)
+#define sub_ddmmss(sh, sl, ah, al, bh, bl)									\
+  do {																		\
+    if (__builtin_constant_p (ah) && (ah) == 0)								\
+      __asm__ ("{sf%I3|subf%I3c} %1,%4,%3\n\t{sfze|subfze} %0,%2"			\
+	       : "=r" (sh), "=&r" (sl) : "r" (bh), "rI" (al), "r" (bl));		\
+    else if (__builtin_constant_p (ah) && (ah) == ~(unsigned long int) 0)	\
+      __asm__ ("{sf%I3|subf%I3c} %1,%4,%3\n\t{sfme|subfme} %0,%2"			\
+	       : "=r" (sh), "=&r" (sl) : "r" (bh), "rI" (al), "r" (bl));		\
+    else if (__builtin_constant_p (bh) && (bh) == 0)						\
+      __asm__ ("{sf%I3|subf%I3c} %1,%4,%3\n\t{ame|addme} %0,%2"				\
+	       : "=r" (sh), "=&r" (sl) : "r" (ah), "rI" (al), "r" (bl));		\
+    else if (__builtin_constant_p (bh) && (bh) == ~(unsigned long int) 0)	\
+      __asm__ ("{sf%I3|subf%I3c} %1,%4,%3\n\t{aze|addze} %0,%2"				\
+	       : "=r" (sh), "=&r" (sl) : "r" (ah), "rI" (al), "r" (bl));		\
+    else																	\
+      __asm__ ("{sf%I4|subf%I4c} %1,%5,%4\n\t{sfe|subfe} %0,%3,%2"			\
+	       : "=r" (sh), "=&r" (sl)											\
+	       : "r" (ah), "r" (bh), "rI" (al), "r" (bl));						\
+  } while (0)
+#define count_leading_zeros(count, x)										\
+  __asm__ ("{cntlz|cntlzw} %0,%1" : "=r" (count) : "r" (x))
+#define COUNT_LEADING_ZEROS_0 32
+#define umul_ppmm(ph, pl, m0, m1)											\
+  do {																		\
+    unsigned long int __m0 = (m0), __m1 = (m1);								\
+    __asm__ ("mulhwu %0,%1,%2" : "=r" (ph) : "%r" (m0), "r" (m1));			\
+    (pl) = __m0 * __m1;														\
+  } while (0)
+#define UMUL_TIME 15
+#define smul_ppmm(ph, pl, m0, m1)											\
+  do {																		\
+    signed long int __m0 = (m0), __m1 = (m1);								\
+    __asm__ ("mulhw %0,%1,%2" : "=r" (ph) : "%r" (m0), "r" (m1));			\
+    (pl) = __m0 * __m1;														\
+  } while (0)
+#define SMUL_TIME 14
+#define UDIV_TIME 120
+#endif /* 32-bit POWER architecture variants. */
+
+#if defined (__ppc64__)
+#define add_ssaaaa(sh, sl, ah, al, bh, bl)									\
+  do {																		\
+    if (__builtin_constant_p (bh) && (bh) == 0)								\
+      __asm__ ("{a%I4|add%I4c} %1,%3,%4\n\t{aze|addze} %0,%2"				\
+	     : "=r" (sh), "=&r" (sl) : "r" (ah), "%r" (al), "rI" (bl));			\
+    else if (__builtin_constant_p (bh) && (bh) == ~(unsigned long long int) 0)	\
+      __asm__ ("{a%I4|add%I4c} %1,%3,%4\n\t{ame|addme} %0,%2"				\
+	     : "=r" (sh), "=&r" (sl) : "r" (ah), "%r" (al), "rI" (bl));			\
+    else																	\
+      __asm__ ("{a%I5|add%I5c} %1,%4,%5\n\t{ae|adde} %0,%2,%3"				\
+	     : "=r" (sh), "=&r" (sl)											\
+	     : "%r" (ah), "r" (bh), "%r" (al), "rI" (bl));						\
+  } while (0)
+#define sub_ddmmss(sh, sl, ah, al, bh, bl)									\
+  do {																		\
+    if (__builtin_constant_p (ah) && (ah) == 0)								\
+      __asm__ ("{sf%I3|subf%I3c} %1,%4,%3\n\t{sfze|subfze} %0,%2"			\
+	       : "=r" (sh), "=&r" (sl) : "r" (bh), "rI" (al), "r" (bl));		\
+    else if (__builtin_constant_p (ah) && (ah) == ~(unsigned long long int) 0)	\
+      __asm__ ("{sf%I3|subf%I3c} %1,%4,%3\n\t{sfme|subfme} %0,%2"			\
+	       : "=r" (sh), "=&r" (sl) : "r" (bh), "rI" (al), "r" (bl));		\
+    else if (__builtin_constant_p (bh) && (bh) == 0)						\
+      __asm__ ("{sf%I3|subf%I3c} %1,%4,%3\n\t{ame|addme} %0,%2"				\
+	       : "=r" (sh), "=&r" (sl) : "r" (ah), "rI" (al), "r" (bl));		\
+    else if (__builtin_constant_p (bh) && (bh) == ~(unsigned long long int) 0)	\
+      __asm__ ("{sf%I3|subf%I3c} %1,%4,%3\n\t{aze|addze} %0,%2"				\
+	       : "=r" (sh), "=&r" (sl) : "r" (ah), "rI" (al), "r" (bl));		\
+    else																	\
+      __asm__ ("{sf%I4|subf%I4c} %1,%5,%4\n\t{sfe|subfe} %0,%3,%2"			\
+	       : "=r" (sh), "=&r" (sl)											\
+	       : "r" (ah), "r" (bh), "rI" (al), "r" (bl));						\
+  } while (0)
+#define count_leading_zeros(count, x) 										\
+  __asm__ ("cntlzd %0,%1" : "=r" (count) : "r" (x))
+#define COUNT_LEADING_ZEROS_0 64
+#define umul_ppmm(ph, pl, m0, m1) 											\
+  do {																		\
+    unsigned long long int __m0 = (m0), __m1 = (m1);						\
+    __asm__ ("mulhdu %0,%1,%2" : "=r" (ph) : "%r" (m0), "r" (m1));			\
+    (pl) = __m0 * __m1;														\
+  } while (0)
+#define UMUL_TIME 15
+#define smul_ppmm(ph, pl, m0, m1)											\
+  do {																		\
+    signed long long int __m0 = (m0), __m1 = (m1);							\
+    __asm__ ("mulhd %0,%1,%2" : "=r" (ph) : "%r" (m0), "r" (m1));			\
+    (pl) = __m0 * __m1;														\
+  } while (0)
+#define SMUL_TIME 14  /* ??? */
+#define UDIV_TIME 120 /* ??? */
+#endif /* 64-bit PowerPC. */
-- 
2.36.1

